Great — let’s build **Module 4: Vision AI — Classification Without Code** as if you were delivering it live in your 24-hour 2022 AI course. I’ll write it as a **full instructor’s script** with explanations, transitions, and demo guidance, so you can use it *verbatim* in a classroom or workshop. It’s designed for a **4-hour session**, mixing theory, demos, and hands-on exercises.

---

## 📘 Module 4: Vision AI – Classification Without Code (4 hours)

**Theme:** Teaching machines to “see” with drag-and-drop tools.
**Learning Goal:** By the end of this module, students will be able to build an image classifier without writing a single line of code, integrate it into a workflow, and understand how visual AI fits into real-world applications.

---

### 🕐 0:00 – 0:15 | Introduction & Framing

**Instructor Script:**
“Welcome back! Up to now, we’ve been dealing with text — classifying it, analyzing sentiment, summarizing it, and building workflows around language. But humans don’t just process language — we also process *visual information*. The ability to recognize patterns in images is something we do naturally from birth.

What if machines could do the same? What if you could upload a folder of product photos, and an AI could instantly label them as defective or safe? Or scan social media photos to categorize brand mentions?

That’s exactly what we’ll learn today: how to give machines the ability to **see, understand, and classify images — without writing code.**”

**Transition:**
“Let’s start by understanding how computers see images in the first place.”

---

### 🧠 0:15 – 0:45 | Theory: How Vision AI Works (Concepts Only)

**Instructor Script:**
“An image is nothing but a grid of pixels — numbers representing color and intensity. AI models, especially convolutional neural networks (CNNs), learn patterns in those numbers to detect shapes, textures, and objects.

There are a few common tasks in computer vision:

* **Image classification:** Identify what category an image belongs to (e.g., cat or dog).
* **Object detection:** Identify and locate objects within an image.
* **Segmentation:** Label every pixel according to what it belongs to (e.g., sky, road, tree).
* **OCR (Optical Character Recognition):** Extract text from images.

Today we’ll focus on **image classification** because it’s the simplest and most common — and it’s where no-code tools are most mature.

Real-world applications include:

* Quality control in manufacturing (good product vs defective)
* Document processing (invoice vs receipt)
* Content moderation (safe vs unsafe)
* Medical imaging (benign vs malignant)”

**Transition:**
“Now let’s look at how we can actually build such a system without writing a single line of code.”

---

### 🧪 0:45 – 1:45 | Demo: Building a Classifier with Teachable Machine

**Instructor Script:**
“Our first tool today is [Teachable Machine](https://teachablemachine.withgoogle.com/) — a simple browser-based platform by Google that lets anyone train image, sound, or pose recognition models by drag-and-drop.”

**Step-by-Step (Show and Narrate):**

1. **Open Teachable Machine.**
   “Go to the site and click ‘Get Started.’ Choose ‘Image Project.’”

2. **Create Classes.**
   “I’m going to build a simple classifier that distinguishes between two types of fruit: apples and bananas. You can choose any two categories — dogs vs cats, defective vs non-defective products, etc.”

3. **Upload Images.**
   “Click on ‘Class 1’ and upload 20–30 images of apples. Then go to ‘Class 2’ and upload 20–30 images of bananas. You can even use your webcam to collect them live.”

4. **Train the Model.**
   “Now click ‘Train Model.’ Teachable Machine will build a neural network behind the scenes — without you writing a single line of code. This might take a minute or two.”

5. **Test the Model.**
   “Upload a new image or use the webcam to see how well it classifies. Notice how confident the model is — that’s the probability score.”

6. **Export the Model.**
   “Once you’re happy, you can download the model or get a shareable link. You can also export it to TensorFlow.js and embed it into a website.”

**Instructor Tip:**
“Discuss limitations — for example, the model might struggle with images in different lighting or with similar-looking fruits. This is an opportunity to talk about dataset diversity and bias.”

---

### 🛠️ 1:45 – 2:30 | Hands-On Lab: Build Your Own Image Classifier

**Student Activity Instructions:**

1. Choose a classification problem relevant to you. Examples:

   * Healthy vs unhealthy plant leaves
   * Formal vs casual clothing
   * Company logos vs competitors’ logos
2. Collect at least 20 images per category (more if possible).
3. Train a Teachable Machine classifier.
4. Test it with new images.
5. Share results with the group — discuss accuracy and edge cases.

**Instructor Coaching:**
Walk around, help with dataset selection, encourage students to think about where such a model could be used in real-world applications.

---

### 🧰 2:30 – 3:15 | Demo 2: Using Lobe.ai for Desktop Training

**Instructor Script:**
“Teachable Machine is great for quick experiments, but if you want more control — like exporting a model to use in apps — tools like **Lobe.ai** make it just as easy but more powerful.”

**Demo Steps:**

1. Install and open Lobe (by Microsoft).
2. Create a new project and import images.
3. Label them and train the model (Lobe does this locally).
4. Test and export as a TensorFlow or ONNX model.
5. Integrate it into a low-code tool or web app.

**Key Discussion Points:**

* Lobe can train on your local machine — good for sensitive data.
* It outputs models you can use in Power Apps, mobile apps, or custom dashboards.

---

### ⚙️ 3:15 – 3:45 | Integration Exercise: Automate Image Processing Workflow

**Instructor Script:**
“Now let’s combine what we’ve learned with automation. We’ll use a simple workflow:

1. New image uploaded to a Google Drive folder.
2. It’s automatically sent to a classifier (via Lobe or API).
3. The label is written to a spreadsheet or sent as an email.”

**Student Exercise:**

* Use Zapier or Power Automate.
* Trigger: New image in a folder.
* Action: Send it to a vision API or classifier endpoint.
* Final step: Save classification result.

**Real-World Use Cases:**

* Automatically tagging product photos in e-commerce.
* Sorting scanned documents into folders.
* Flagging NSFW content uploads automatically.

---

### 🏁 3:45 – 4:00 | Wrap-Up and Reflection

**Instructor Script:**
“Let’s recap what we accomplished today:

* You learned how computers ‘see’ and how image classification works.
* You built your own classifier without writing a single line of code.
* You explored two real-world tools: Teachable Machine for rapid prototyping and Lobe.ai for production-ready models.
* And you connected vision AI into a workflow to automate real tasks.

Back in 2022, this was cutting-edge — companies were spending millions building tools like this to sort images, detect quality issues, and moderate content. We did it here today in less than four hours.

In our next module, we’ll combine everything — text, vision, and automation — into end-to-end workflows that solve real business problems.”

---

✅ **End-of-Module Assignment (Optional Homework):**
Create a small project where an image classifier labels images from a dataset and automatically organizes them into folders or logs them into a Google Sheet. Document the process and prepare a short 2-minute demo for the next session.