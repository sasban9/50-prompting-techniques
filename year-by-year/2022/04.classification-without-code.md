Great â€” letâ€™s build **Module 4: Vision AI â€” Classification Without Code** as if you were delivering it live in your 24-hour 2022 AI course. Iâ€™ll write it as a **full instructorâ€™s script** with explanations, transitions, and demo guidance, so you can use it *verbatim* in a classroom or workshop. Itâ€™s designed for a **4-hour session**, mixing theory, demos, and hands-on exercises.

---

## ğŸ“˜ Module 4: Vision AI â€“ Classification Without Code (4 hours)

**Theme:** Teaching machines to â€œseeâ€ with drag-and-drop tools.
**Learning Goal:** By the end of this module, students will be able to build an image classifier without writing a single line of code, integrate it into a workflow, and understand how visual AI fits into real-world applications.

---

### ğŸ• 0:00 â€“ 0:15 | Introduction & Framing

**Instructor Script:**
â€œWelcome back! Up to now, weâ€™ve been dealing with text â€” classifying it, analyzing sentiment, summarizing it, and building workflows around language. But humans donâ€™t just process language â€” we also process *visual information*. The ability to recognize patterns in images is something we do naturally from birth.

What if machines could do the same? What if you could upload a folder of product photos, and an AI could instantly label them as defective or safe? Or scan social media photos to categorize brand mentions?

Thatâ€™s exactly what weâ€™ll learn today: how to give machines the ability to **see, understand, and classify images â€” without writing code.**â€

**Transition:**
â€œLetâ€™s start by understanding how computers see images in the first place.â€

---

### ğŸ§  0:15 â€“ 0:45 | Theory: How Vision AI Works (Concepts Only)

**Instructor Script:**
â€œAn image is nothing but a grid of pixels â€” numbers representing color and intensity. AI models, especially convolutional neural networks (CNNs), learn patterns in those numbers to detect shapes, textures, and objects.

There are a few common tasks in computer vision:

* **Image classification:** Identify what category an image belongs to (e.g., cat or dog).
* **Object detection:** Identify and locate objects within an image.
* **Segmentation:** Label every pixel according to what it belongs to (e.g., sky, road, tree).
* **OCR (Optical Character Recognition):** Extract text from images.

Today weâ€™ll focus on **image classification** because itâ€™s the simplest and most common â€” and itâ€™s where no-code tools are most mature.

Real-world applications include:

* Quality control in manufacturing (good product vs defective)
* Document processing (invoice vs receipt)
* Content moderation (safe vs unsafe)
* Medical imaging (benign vs malignant)â€

**Transition:**
â€œNow letâ€™s look at how we can actually build such a system without writing a single line of code.â€

---

### ğŸ§ª 0:45 â€“ 1:45 | Demo: Building a Classifier with Teachable Machine

**Instructor Script:**
â€œOur first tool today is [Teachable Machine](https://teachablemachine.withgoogle.com/) â€” a simple browser-based platform by Google that lets anyone train image, sound, or pose recognition models by drag-and-drop.â€

**Step-by-Step (Show and Narrate):**

1. **Open Teachable Machine.**
   â€œGo to the site and click â€˜Get Started.â€™ Choose â€˜Image Project.â€™â€

2. **Create Classes.**
   â€œIâ€™m going to build a simple classifier that distinguishes between two types of fruit: apples and bananas. You can choose any two categories â€” dogs vs cats, defective vs non-defective products, etc.â€

3. **Upload Images.**
   â€œClick on â€˜Class 1â€™ and upload 20â€“30 images of apples. Then go to â€˜Class 2â€™ and upload 20â€“30 images of bananas. You can even use your webcam to collect them live.â€

4. **Train the Model.**
   â€œNow click â€˜Train Model.â€™ Teachable Machine will build a neural network behind the scenes â€” without you writing a single line of code. This might take a minute or two.â€

5. **Test the Model.**
   â€œUpload a new image or use the webcam to see how well it classifies. Notice how confident the model is â€” thatâ€™s the probability score.â€

6. **Export the Model.**
   â€œOnce youâ€™re happy, you can download the model or get a shareable link. You can also export it to TensorFlow.js and embed it into a website.â€

**Instructor Tip:**
â€œDiscuss limitations â€” for example, the model might struggle with images in different lighting or with similar-looking fruits. This is an opportunity to talk about dataset diversity and bias.â€

---

### ğŸ› ï¸ 1:45 â€“ 2:30 | Hands-On Lab: Build Your Own Image Classifier

**Student Activity Instructions:**

1. Choose a classification problem relevant to you. Examples:

   * Healthy vs unhealthy plant leaves
   * Formal vs casual clothing
   * Company logos vs competitorsâ€™ logos
2. Collect at least 20 images per category (more if possible).
3. Train a Teachable Machine classifier.
4. Test it with new images.
5. Share results with the group â€” discuss accuracy and edge cases.

**Instructor Coaching:**
Walk around, help with dataset selection, encourage students to think about where such a model could be used in real-world applications.

---

### ğŸ§° 2:30 â€“ 3:15 | Demo 2: Using Lobe.ai for Desktop Training

**Instructor Script:**
â€œTeachable Machine is great for quick experiments, but if you want more control â€” like exporting a model to use in apps â€” tools like **Lobe.ai** make it just as easy but more powerful.â€

**Demo Steps:**

1. Install and open Lobe (by Microsoft).
2. Create a new project and import images.
3. Label them and train the model (Lobe does this locally).
4. Test and export as a TensorFlow or ONNX model.
5. Integrate it into a low-code tool or web app.

**Key Discussion Points:**

* Lobe can train on your local machine â€” good for sensitive data.
* It outputs models you can use in Power Apps, mobile apps, or custom dashboards.

---

### âš™ï¸ 3:15 â€“ 3:45 | Integration Exercise: Automate Image Processing Workflow

**Instructor Script:**
â€œNow letâ€™s combine what weâ€™ve learned with automation. Weâ€™ll use a simple workflow:

1. New image uploaded to a Google Drive folder.
2. Itâ€™s automatically sent to a classifier (via Lobe or API).
3. The label is written to a spreadsheet or sent as an email.â€

**Student Exercise:**

* Use Zapier or Power Automate.
* Trigger: New image in a folder.
* Action: Send it to a vision API or classifier endpoint.
* Final step: Save classification result.

**Real-World Use Cases:**

* Automatically tagging product photos in e-commerce.
* Sorting scanned documents into folders.
* Flagging NSFW content uploads automatically.

---

### ğŸ 3:45 â€“ 4:00 | Wrap-Up and Reflection

**Instructor Script:**
â€œLetâ€™s recap what we accomplished today:

* You learned how computers â€˜seeâ€™ and how image classification works.
* You built your own classifier without writing a single line of code.
* You explored two real-world tools: Teachable Machine for rapid prototyping and Lobe.ai for production-ready models.
* And you connected vision AI into a workflow to automate real tasks.

Back in 2022, this was cutting-edge â€” companies were spending millions building tools like this to sort images, detect quality issues, and moderate content. We did it here today in less than four hours.

In our next module, weâ€™ll combine everything â€” text, vision, and automation â€” into end-to-end workflows that solve real business problems.â€

---

âœ… **End-of-Module Assignment (Optional Homework):**
Create a small project where an image classifier labels images from a dataset and automatically organizes them into folders or logs them into a Google Sheet. Document the process and prepare a short 2-minute demo for the next session.